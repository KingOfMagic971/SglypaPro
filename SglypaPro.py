# -*- coding: utf-8 -*-
# meta developer: @Rezoxss
# scope: hikka_only

from .. import loader, utils
import logging
import random
import re
import asyncio
from collections import defaultdict
import torch
import torch.nn as nn
from transformers import GPT2LMHeadModel, GPT2Tokenizer

logger = logging.getLogger(__name__)

@loader.tds
class NeuralSglypaMod(loader.Module):
    """–ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –º–æ–¥—É–ª—å —Å–≥–ª—ã–ø—ã —Å PyTorch –∏ GPT"""
    
    strings = {
        "name": "NeuralSglypa",
        "on": "‚úÖ –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π —Ä–µ–∂–∏–º —Å–≥–ª—ã–ø—ã –≤–∫–ª—é—á–µ–Ω! GPT –º–æ–¥–µ–ª—å –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞",
        "off": "‚ùå –†–µ–∂–∏–º —Å–≥–ª—ã–ø—ã –≤—ã–∫–ª—é—á–µ–Ω",
        "already_on": "‚ö†Ô∏è –†–µ–∂–∏–º —É–∂–µ –≤–∫–ª—é—á–µ–Ω",
        "already_off": "‚ö†Ô∏è –†–µ–∂–∏–º —É–∂–µ –≤—ã–∫–ª—é—á–µ–Ω",
        "model_loading": "üîÑ –ó–∞–≥—Ä—É–∂–∞—é –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—É—é –º–æ–¥–µ–ª—å...",
        "model_ready": "‚úÖ GPT –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏!",
        "model_error": "‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"
    }

    def __init__(self):
        self.config = loader.ModuleConfig(
            loader.ConfigValue(
                "reply_chance",
                40,
                "–®–∞–Ω—Å –æ—Ç–≤–µ—Ç–∞ –≤ %",
                validator=loader.validators.Integer(minimum=1, maximum=100)
            ),
            loader.ConfigValue(
                "use_neural",
                True,
                "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å",
                validator=loader.validators.Boolean()
            ),
            loader.ConfigValue(
                "temperature",
                0.9,
                "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏",
                validator=loader.validators.Float(minimum=0.1, maximum=2.0)
            )
        )
        self.active_chats = set()
        self.chat_history = defaultdict(list)
        self.model = None
        self.tokenizer = None
        self.model_loaded = False

    async def client_ready(self, client, db):
        self._client = client
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Ñ–æ–Ω–µ
        asyncio.create_task(self.load_model())

    async def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—É—é –º–æ–¥–µ–ª—å"""
        try:
            logger.info(self.strings["model_loading"])
            self.tokenizer = GPT2Tokenizer.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.model = GPT2LMHeadModel.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')
            self.model_loaded = True
            logger.info(self.strings["model_ready"])
        except Exception as e:
            logger.error(f"{self.strings['model_error']}: {e}")
            self.model_loaded = False

    def add_to_history(self, chat_id, text):
        """–î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        if text and len(text) > 2:
            words = re.findall(r'\b[–∞-—è—ë]{2,}\b', text.lower())
            for word in words:
                if word not in ['—ç—Ç–æ', '–≤–æ—Ç', '–∫–∞–∫', '—á—Ç–æ', '—Ç–∞–º', '–∑–¥–µ—Å—å']:
                    self.chat_history[chat_id].append(word)

    def generate_neural_sglypa(self, chat_id):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–≥–ª—ã–ø—É —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å"""
        if not self.model_loaded:
            return self.generate_fallback_sglypa(chat_id)
        
        try:
            # –ë–µ—Ä–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
            context_words = []
            if chat_id in self.chat_history and self.chat_history[chat_id]:
                context_words = random.sample(self.chat_history[chat_id], 
                                           min(5, len(self.chat_history[chat_id])))
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            prompt = "–°–≥–ª—ã–ø–∞: " + " ".join(context_words) + " "
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
            inputs = self.tokenizer.encode(prompt, return_tensors='pt', max_length=50, truncation=True)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=random.randint(20, 50),
                    num_return_sequences=1,
                    temperature=self.config["temperature"],
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    top_k=50,
                    top_p=0.9,
                    repetition_penalty=1.2
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
            result = generated_text.replace(prompt, "").strip()
            
            # –ß–∏—Å—Ç–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = re.sub(r'[^–∞-—è—ë–ê-–Ø–Å\s]', '', result)
            result = ' '.join(result.split()[:random.randint(3, 8)])
            
            return result.capitalize()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {e}")
            return self.generate_fallback_sglypa(chat_id)

    def generate_fallback_sglypa(self, chat_id):
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –µ—Å–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"""
        patterns = [
            "{} {} {}", "{} {} {} {}", "{} {} {} {} {}", 
            "–±–ª—è–¥—å {} {}", "–Ω–∞—Ö—É–π {} {}", "–ø–∏–∑–¥–µ—Ü {} {}",
            "—ë–±–∞–Ω—ã–π {} {}", "–∑–∞–µ–±–∏—Å—å {} {}", "–æ—Ç—ä–µ–±–∏—Å—å {} {}",
            "–∞ –≤–æ—Ç –∏ {} {}", "–∏ —Ç—É—Ç {} {}", "–≤–Ω–µ–∑–∞–ø–Ω–æ {} {}"
        ]
        
        if chat_id in self.chat_history and self.chat_history[chat_id]:
            words = list(self.chat_history[chat_id])
        else:
            words = ["—Å–≥–ª—ã–ø–∞", "–ø–∏–¥–æ—Ä", "–∂–æ–ø–∞", "—Ö—É–π", "–ø–∏–∑–¥–∞", "–µ–±–ª–∞–Ω"]
        
        pattern = random.choice(patterns)
        num_slots = pattern.count("{}")
        
        result = pattern
        for _ in range(num_slots):
            if words:
                word = random.choice(words)
                result = result.replace("{}", word, 1)
            else:
                result = result.replace("{}", "—Å–≥–ª—ã–ø–∞", 1)
        
        return result.capitalize()

    @loader.command()
    async def sglypa(self, message):
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–≥–ª—ã–ø—É —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç—å - .sglypa [on/off]"""
        args = utils.get_args_raw(message)
        chat_id = utils.get_chat_id(message)
        
        if message.text:
            self.add_to_history(chat_id, message.text)

        if not args:
            if self.config["use_neural"] and self.model_loaded:
                sglypa_text = self.generate_neural_sglypa(chat_id)
            else:
                sglypa_text = self.generate_fallback_sglypa(chat_id)
            await utils.answer(message, sglypa_text)
            return
            
        if args.lower() == "on":
            if chat_id in self.active_chats:
                await utils.answer(message, self.strings("already_on"))
            else:
                self.active_chats.add(chat_id)
                await utils.answer(message, self.strings("on"))
                
        elif args.lower() == "off":
            if chat_id not in self.active_chats:
                await utils.answer(message, self.strings("already_off"))
            else:
                self.active_chats.discard(chat_id)
                await utils.answer(message, self.strings("off"))
                
        elif args.lower() == "status":
            status = "‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞" if self.model_loaded else "‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
            await utils.answer(message, f"–°—Ç–∞—Ç—É—Å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {status}")
                
        else:
            if self.config["use_neural"] and self.model_loaded:
                sglypa_text = self.generate_neural_sglypa(chat_id)
            else:
                sglypa_text = self.generate_fallback_sglypa(chat_id)
            await utils.answer(message, sglypa_text)

    @loader.watcher()
    async def watcher(self, message):
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏"""
        chat_id = utils.get_chat_id(message)
        
        if chat_id not in self.active_chats:
            return
            
        if not message.text or message.out or message.text.startswith('.'):
            return
            
        self.add_to_history(chat_id, message.text)
        
        # –†–µ–∞–≥–∏—Ä—É–µ–º –Ω–∞ —Å–ª–æ–≤–æ "—Å–≥–ª—ã–ø–∞"
        if re.search(r'—Å–≥–ª—ã–ø–∞', message.text, re.IGNORECASE):
            if self.config["use_neural"] and self.model_loaded:
                sglypa_text = self.generate_neural_sglypa(chat_id)
            else:
                sglypa_text = self.generate_fallback_sglypa(chat_id)
            await message.reply(sglypa_text)
            return
            
        # –û–±—ã—á–Ω—ã–π —à–∞–Ω—Å –æ—Ç–≤–µ—Ç–∞
        if random.randint(1, 100) <= self.config["reply_chance"]:
            if self.config["use_neural"] and self.model_loaded:
                sglypa_text = self.generate_neural_sglypa(chat_id)
            else:
                sglypa_text = self.generate_fallback_sglypa(chat_id)
            await message.reply(sglypa_text)

    async def on_unload(self):
        """–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        if self.model:
            del self.model
        if self.tokenizer:
            del self.tokenizer
        self.active_chats.clear()
        self.chat_history.clear()